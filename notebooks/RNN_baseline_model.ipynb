{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804ef6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from droughts_modelling.data import DataFunctions\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc1f83",
   "metadata": {},
   "source": [
    "## Tensorflow window class method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c706dd3",
   "metadata": {},
   "source": [
    "DataFunctions class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565364bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFunctions().light_weekly_aggregate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa7e8e",
   "metadata": {},
   "source": [
    "To dos:\n",
    "1. Train locally a v small DL model with a v small part of the dataset that has been processed using the window generator\n",
    "2. Work out what Alex was predicting... was it the next week? was it further into the future?\n",
    "3. Code up and implement in the DL trainer file\n",
    "- Hook up the DL trainer file to GCP\n",
    "- Work in the geolocation data\n",
    "- Think about using autoregressive method to improve performance\n",
    "- Get the Keras wrapper working so we can use pipelines\n",
    "4. When it comes to fine tuning think about using the autoregressive method in tensorflow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7128a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self,data,input_width,label_width,shift,label_columns=None):\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "            self.column_indices = {name: i for i, name in enumerate(self.data.columns)}\n",
    "\n",
    "    \n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    \n",
    "    def split_window(self, list_of_consecutive_inputs_w_labels):\n",
    "        inputs = list_of_consecutive_inputs_w_labels[:, self.input_slice, :]\n",
    "        labels = list_of_consecutive_inputs_w_labels[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns],axis=-1)\n",
    "\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self):\n",
    "        data = np.array(self.data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(data=data,targets=None,\n",
    "                                                              sequence_length=self.total_window_size,\n",
    "          sequence_stride=1,\n",
    "          shuffle=True,batch_size=32)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a90d6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = WindowGenerator(data,input_width=6, label_width=1, shift=1,label_columns=['score_max']).make_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e897848d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 6, 21), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3c300d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164736"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_data) * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9576bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164724"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5be32126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (batch, time, features): (32, 6, 21)\n",
      "Labels shape (batch, time, features): (32, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "for example_inputs, example_labels in preprocessed_data.take(1):\n",
    "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "    print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97eac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearning:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = DataFunctions().light_weekly_aggregate()\n",
    "        self.features = self.data.drop(columns=['fips_','week_num_','score_max']).columns\n",
    "        \n",
    "    def robust(self):\n",
    "        df = self.data.copy()\n",
    "        for f in self.features:\n",
    "            median = np.median(df[f])\n",
    "            IQR = np.subtract(*np.percentile(df[f], [75, 25]))\n",
    "            df[f] = df[f].map(lambda x: (x-median)/IQR)\n",
    "        \n",
    "        self.scaled_data = df\n",
    "            \n",
    "    def preprocess(self):\n",
    "        self.robust()\n",
    "        self.preprocessed_data = WindowGenerator(self.scaled_data,input_width=6, label_width=1, shift=1,label_columns=['score_max']).make_dataset()\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        self.model = models.Sequential()\n",
    "        self.model.add(layers.LSTM(20))\n",
    "        self.model.add(layers.Dense(6,activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy','recall'])\n",
    "    \n",
    "    def train_model(self):\n",
    "        self.initialize_model()\n",
    "        self.preprocess()\n",
    "        self.model.fit(self.preprocessed_data,epochs=1,batch_size=16,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd48ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from droughts_modelling.DL_trainer import DeepLearning as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63245ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 22:40:37.195415: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-31 22:40:39.914185: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5148/5148 [==============================] - 53s 10ms/step - loss: 0.0000e+00 - accuracy: 0.0729\n"
     ]
    }
   ],
   "source": [
    "dl().train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e4ffc",
   "metadata": {},
   "source": [
    "## Manual coding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab8a6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subsample_sequence(df, length):\n",
    "    df_sample = df.sample(length)\n",
    "    X_sample = df_sample.iloc[:-1].copy()\n",
    "    y_sample = float(df_sample[['score_max']].iloc[-1])\n",
    "    \n",
    "    return X_sample, y_sample\n",
    "\n",
    "def get_X_y(df, n_sequences, length):\n",
    "    X = [split_subsample_sequence(df, length)[0] for n in range(n_sequences)]\n",
    "    y = [split_subsample_sequence(df, length)[1] for n in range(n_sequences)]\n",
    "    \n",
    "    \n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db5417a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df984cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_X_y(d, 2000, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52ab5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subsample_sequence(df, length):\n",
    "    df_sample = df.sample(length,random_state=1)\n",
    "    X_sample = df_sample.iloc[:-1].copy()\n",
    "    y_sample = float(df_sample[['score_max']].iloc[-1])\n",
    "    \n",
    "    return X_sample, y_sample\n",
    "\n",
    "def get_X_y(df, n_sequences,length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for n in range(n_sequences):\n",
    "        sequence = split_subsample_sequence(df, length)\n",
    "        X.append(sequence[0])\n",
    "        y.append(sequence[1])\n",
    "    \n",
    "    return np.array(X),np.array(y)\n",
    "\n",
    "def get_X_y_by_geolocation(df, n_sequences, length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for fip in sorted(set(df['fips_'])):\n",
    "        fip_df = df[df['fips_'] == fip]\n",
    "        sequences = get_X_y(fip_df,n_sequences,length)\n",
    "        X.append(sequences[0])\n",
    "        y.append(sequences[1])\n",
    "        \n",
    "\n",
    "    return np.array(X),np.array(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "droughts",
   "language": "python",
   "name": "droughts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
