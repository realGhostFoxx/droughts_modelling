{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e71628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from droughts_modelling.data import DataFunctions\n",
    "from droughts_modelling.window_gen import WindowGenerator\n",
    "from droughts_modelling.updated_DL_trainer import DeepLearning2\n",
    "import tensorflow\n",
    "from tensorflow.keras import models, layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782f37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DataFunctions().light_weekly_aggregate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3114d1e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metawindow = WindowGenerator(test_data[test_data['fips_']==1001],input_width=6,label_width=6,shift=1,label_columns=[\"score_max\"]).make_dataset()\n",
    "for fips in set(test_data['fips_']):\n",
    "    if fips != 1001:\n",
    "        df = test_data[test_data['fips_'] == fips]\n",
    "        window = WindowGenerator(df,input_width=6,label_width=6,shift=1,label_columns=[\"score_max\"]).make_dataset()\n",
    "        metawindow = metawindow.concatenate(window)\n",
    "    \n",
    "metawindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c01af",
   "metadata": {},
   "outputs": [],
   "source": [
    "metawindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fedfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.LSTM(32,return_sequences=True,activation='tanh'))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38699f51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(metawindow,epochs=1,batch_size=32,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f250881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import models,layers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from droughts_modelling.data import DataFunctions\n",
    "from droughts_modelling.window_gen import WindowGenerator\n",
    "import numpy as np\n",
    "\n",
    "class DeepLearning2():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_data = DataFunctions().light_weekly_aggregate_train()\n",
    "        self.test_data = DataFunctions().light_weekly_aggregate_test()\n",
    "        self.features = self.train_data.drop(columns=['fips_','year_','week_num_','score_max']).columns\n",
    "    \n",
    "    #Data Scaling: Train and Test\n",
    "    def robust(self):\n",
    "        train_df = self.train_data.copy()\n",
    "        test_df = self.test_data.copy()\n",
    "        for f in self.features:\n",
    "            train_median = np.median(train_df[f])\n",
    "            train_iqr = np.subtract(*np.percentile(train_df[f], [75, 25]))\n",
    "            train_df[f] = train_df[f].map(lambda x: (x-train_median)/train_iqr)\n",
    "            test_df[f] = test_df[f].map(lambda x: (x-train_median)/train_iqr)\n",
    "            \n",
    "        self.train_df_robust = train_df\n",
    "        self.test_df_robust = test_df\n",
    "        print('robust done')\n",
    "    \n",
    "    \n",
    "    #One Hot Encoding: Train and Test\n",
    "    \n",
    "    def ohe(self):\n",
    "        self.robust()\n",
    "        train_df = self.train_df_robust.copy()\n",
    "        test_df = self.test_df_robust.copy()\n",
    "        \n",
    "        train_ohe = OneHotEncoder(sparse = False)\n",
    "        test_ohe = OneHotEncoder(sparse = False)\n",
    "        \n",
    "        train_ohe.fit(train_df[['score_max']])\n",
    "        test_ohe.fit(test_df[['score_max']])\n",
    "        \n",
    "        scoremax_encoded_train = train_ohe.transform(train_df[['score_max']])\n",
    "        scoremax_encoded_test = test_ohe.transform(test_df[['score_max']])\n",
    "        \n",
    "        train_df[\"score_max_0\"],train_df[\"score_max_1\"],train_df['score_max_2'],train_df['score_max_3'],train_df['score_max_4'],train_df['score_max_5'] = scoremax_encoded_train.T \n",
    "        test_df[\"score_max_0\"],test_df[\"score_max_1\"],test_df['score_max_2'],test_df['score_max_3'],test_df['score_max_4'],test_df['score_max_5'] = scoremax_encoded_test.T \n",
    "        \n",
    "        self.train_df_robust_ohe = train_df.drop(columns=['score_max'])\n",
    "        self.test_df_robust_ohe = test_df.drop(columns=['score_max'])\n",
    "        print('ohe_done')\n",
    "        \n",
    "    #Generating Windows: Train and Test    \n",
    "        \n",
    "    def window(self):\n",
    "        self.ohe()\n",
    "        train_df = self.train_df_robust_ohe\n",
    "        test_df = self.test_df_robust_ohe\n",
    "        \n",
    "        def fip_splitter(df):\n",
    "            window = WindowGenerator(df[df['fips_'] == 1001],input_width=6,label_width=6,shift=1,label_columns=[\"score_max_0\",\"score_max_1\",\"score_max_2\",\"score_max_3\",\"score_max_4\",\"score_max_5\"]).make_dataset()\n",
    "            for fips in set(df['fips_']):\n",
    "                if fips != 1001:\n",
    "                    fip_df = df[df['fips_'] == fips]\n",
    "                    fip_window = WindowGenerator(fip_df,input_width=6,label_width=6,shift=1,label_columns=[\"score_max_0\",\"score_max_1\",\"score_max_2\",\"score_max_3\",\"score_max_4\",\"score_max_5\"]).make_dataset()\n",
    "                    window = window.concatenate(fip_window)\n",
    "            return window\n",
    "            \n",
    "        self.train_metawindow = fip_splitter(train_df)\n",
    "        self.test_metawindow = fip_splitter(test_df)\n",
    "        return self.test_metawindow\n",
    "    \n",
    "    \n",
    "    #Model + evaluation\n",
    "    def initialize_model(self):\n",
    "        self.model = models.Sequential()\n",
    "        self.model.add(layers.LSTM(32,return_sequences=True,activation='tanh'))\n",
    "        self.model.add(layers.LSTM(32,return_sequences=True,activation='tanh'))\n",
    "        self.model.add(layers.Dense(20,activation='relu'))\n",
    "        self.model.add(layers.Dense(6,activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "        \n",
    "    def train_evaluate_model(self):\n",
    "        self.initialize_model()\n",
    "        self.window()\n",
    "        self.model.fit(self.train_metawindow,epochs=1,batch_size=32,verbose=0)\n",
    "        self.model.evaluate(self.test_metawindow,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a6fc7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust done\n",
      "ohe_done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset shapes: ((None, 6, 29), (None, 6, 6)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeepLearning2().window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d870e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "train_full_path = os.path.join(file_path,'realGhostFoxx','droughts_modelling', 'raw_data', 'train_timeseries.csv')\n",
    "validate_full_path = os.path.join(file_path,'realGhostFoxx','droughts_modelling', 'raw_data', 'validation_timeseries.csv')\n",
    "test_full_path = os.path.join(file_path,'realGhostFoxx','droughts_modelling', 'raw_data', 'test_timeseries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c660fc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hughlupson/code'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c1d2536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hughlupson/code/realGhostFoxx/droughts_modelling/raw_data/train_timeseries.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0bbf548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hughlupson/code/realGhostFoxx/droughts_modelling/raw_data/validation_timeseries.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "793f976d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hughlupson/code/realGhostFoxx/droughts_modelling/raw_data/test_timeseries.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2610026",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "train_full_path = os.path.join(file_path,'realGhostFoxx','droughts_modelling', 'raw_data', 'train_timeseries.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d4c9cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hughlupson/code/realGhostFoxx/droughts_modelling/raw_data/train_timeseries.csv'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee606213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import models,layers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from droughts_modelling.data import DataFunctions\n",
    "from droughts_modelling.window_gen import WindowGenerator\n",
    "import numpy as np\n",
    "\n",
    "class DeepLearning2():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_data = DataFunctions().light_weekly_aggregate_train()\n",
    "        self.test_data = DataFunctions().light_weekly_aggregate_test()\n",
    "        self.features = self.train_data.drop(columns=['fips_','year_','week_num_','score_max']).columns\n",
    "    \n",
    "    #Data Scaling: Train and Test\n",
    "    def robust(self):\n",
    "        train_df = self.train_data.copy()\n",
    "        test_df = self.test_data.copy()\n",
    "        for f in self.features:\n",
    "            train_median = np.median(train_df[f])\n",
    "            train_iqr = np.subtract(*np.percentile(train_df[f], [75, 25]))\n",
    "            train_df[f] = train_df[f].map(lambda x: (x-train_median)/train_iqr)\n",
    "            test_df[f] = test_df[f].map(lambda x: (x-train_median)/train_iqr)\n",
    "            \n",
    "        self.train_df_robust = train_df\n",
    "        self.test_df_robust = test_df\n",
    "    \n",
    "    \n",
    "    #One Hot Encoding: Train and Test\n",
    "    \n",
    "    def ohe(self):\n",
    "        self.robust()\n",
    "        train_df = self.train_df_robust.copy()\n",
    "        test_df = self.test_df_robust.copy()\n",
    "        \n",
    "        train_ohe = OneHotEncoder(sparse = False)\n",
    "        test_ohe = OneHotEncoder(sparse = False)\n",
    "        \n",
    "        train_ohe.fit(train_df[['score_max']])\n",
    "        test_ohe.fit(test_df[['score_max']])\n",
    "        \n",
    "        scoremax_encoded_train = train_ohe.transform(train_df[['score_max']])\n",
    "        scoremax_encoded_test = test_ohe.transform(test_df[['score_max']])\n",
    "        \n",
    "        train_df[\"score_max_0\"],train_df[\"score_max_1\"],train_df['score_max_2'],train_df['score_max_3'],train_df['score_max_4'],train_df['score_max_5'] = scoremax_encoded_train.T \n",
    "        test_df[\"score_max_0\"],test_df[\"score_max_1\"],test_df['score_max_2'],test_df['score_max_3'],test_df['score_max_4'],test_df['score_max_5'] = scoremax_encoded_test.T \n",
    "        \n",
    "        self.train_df_robust_ohe = train_df.drop(columns=['score_max'])\n",
    "        self.test_df_robust_ohe = test_df.drop(columns=['score_max'])\n",
    "        \n",
    "    #Generating Windows: Train and Test    \n",
    "        \n",
    "    def window(self):\n",
    "        self.ohe()\n",
    "        train_df = self.train_df_robust_ohe\n",
    "        test_df = self.test_df_robust_ohe\n",
    "        \n",
    "        def fip_splitter(df):\n",
    "            window = WindowGenerator(df[df['fips_'] == 1001],input_width=6,label_width=6,shift=1,label_columns=[\"score_max_0\",\"score_max_1\",\"score_max_2\",\"score_max_3\",\"score_max_4\",\"score_max_5\"]).make_dataset()\n",
    "            for fips in set(df['fips_']):\n",
    "                if fips != 1001:\n",
    "                    fip_df = df[df['fips_'] == fips]\n",
    "                    fip_window = WindowGenerator(fip_df,input_width=6,label_width=6,shift=1,label_columns=[\"score_max_0\",\"score_max_1\",\"score_max_2\",\"score_max_3\",\"score_max_4\",\"score_max_5\"]).make_dataset()\n",
    "                    window = window.concatenate(fip_window)\n",
    "            return window\n",
    "            \n",
    "        self.train_metawindow = fip_splitter(train_df)\n",
    "        self.test_metawindow = fip_splitter(test_df)\n",
    "        return self.test_metawindow\n",
    "    \n",
    "    #Model + evaluation\n",
    "    def initialize_model(self):\n",
    "        self.model = models.Sequential()\n",
    "        self.model.add(layers.LSTM(32,return_sequences=True,activation='tanh'))\n",
    "        self.model.add(layers.LSTM(32,return_sequences=True,activation='tanh'))\n",
    "        self.model.add(layers.Dense(20,activation='relu'))\n",
    "        self.model.add(layers.Dense(6,activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "        \n",
    "    def train_evaluate_model(self):\n",
    "        self.initialize_model()\n",
    "        self.window()\n",
    "        self.model.fit(self.train_metawindow,epochs=1,batch_size=32,verbose=0)\n",
    "        self.model.evaluate(self.test_metawindow,verbose=0)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    DeepLearning2().train_evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030031a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import ast\n",
    "\n",
    "class DataFunctions:\n",
    "    \n",
    "    def __init__(self):\n",
    "        file_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "        full_path_train = os.path.join(file_path,'realGhostFoxx','droughts_modelling', 'raw_data', 'train_timeseries.csv')\n",
    "        full_path_validate = os.path.join(file_path,'realGhostFoxx','droughts_modelling', 'raw_data', 'validation_timeseries.csv')\n",
    "        full_path_test = os.path.join(file_path,'realGhostFoxx','droughts_modelling', 'raw_data', 'test_timeseries.csv')\n",
    "        self.train_data = pd.read_csv(full_path_train)[2:]\n",
    "        self.validation_data = pd.read_csv(full_path_validate)[1:]\n",
    "        self.test_data = pd.read_csv(full_path_test)[6:]\n",
    "        \n",
    "        self.fips_path = os.path.join(file_path,'realGhostFoxx','droughts_modelling', 'raw_data', 'fips_dict.csv')\n",
    "        self.fips_dict = pd.read_csv(self.fips_path,index_col=[0])\n",
    "    \n",
    "    def train_last_2_years(self):\n",
    "        df = self.train_data\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        temp_df = df[df['date'] >= '2015-01-01']\n",
    "    \n",
    "        return temp_df\n",
    "\n",
    "    def weekly_aggregate(self):\n",
    "        df = self.train_data\n",
    "        \n",
    "        #first create new features: month, weekday, weeknum\n",
    "        df['year'] = pd.to_datetime(df['date']).dt.isocalendar().year\n",
    "        df['week_num'] = pd.to_datetime(df['date']).dt.isocalendar().week\n",
    "        \n",
    "        #then encode the score as a new feature - not sure if we'll need it \n",
    "        df['score_day'] = df['score'].apply(lambda x: 'yes' if pd.notnull(x) == True else '')\n",
    "\n",
    "        #then start aggregating by fips, month, week_num\n",
    "        aggregated_data_train = df.groupby(['fips','year','week_num']).agg(\n",
    "                                        {'PRECTOT': ['min', 'mean', 'std'],\n",
    "                                        'PS': ['min', 'mean', 'std'],\n",
    "                                        'QV2M': ['min', 'mean', 'std'],\n",
    "                                        'T2M': ['min', 'mean', 'std'],\n",
    "                                        'T2MDEW': ['min', 'mean', 'std'],\n",
    "                                        'T2MWET': ['min', 'mean', 'std'],\n",
    "                                        'T2M_MAX': ['min', 'mean', 'std'],\n",
    "                                        'T2M_MIN': ['min', 'mean', 'std'],\n",
    "                                        'T2M_RANGE': ['min', 'mean', 'std'],\n",
    "                                         'TS': ['min', 'mean', 'std'],\n",
    "                                         'WS10M': ['min', 'mean', 'std'],\n",
    "                                         'WS10M_MAX': ['min', 'mean', 'std'],\n",
    "                                         'WS10M_MIN': ['min', 'mean', 'std'],\n",
    "                                         'WS10M_RANGE': ['min', 'mean', 'std'],\n",
    "                                         'WS50M': ['min', 'mean', 'std'],\n",
    "                                         'WS50M_MAX': ['min', 'mean', 'std'],\n",
    "                                         'WS50M_MIN': ['min', 'mean', 'std'],\n",
    "                                         'WS50M_RANGE': ['min', 'mean', 'std'],\n",
    "                                         'score': 'max'}).reset_index().sort_values(['fips','year','week_num'])\n",
    "\n",
    "        #finally, remove the multiindex from aggregated data_train so it looks neat and has flat column name structure\n",
    "        #Then round scores to nearest integer\n",
    "        aggregated_data_train.columns = ['_'.join(col) for col in aggregated_data_train.columns.values]\n",
    "        aggregated_data_train['score_max'] = aggregated_data_train['score_max'].map(lambda x: np.round(x))\n",
    "        \n",
    "        return aggregated_data_train.dropna()\n",
    "    \n",
    "    def light_weekly_aggregate_train(self):\n",
    "        df = self.train_data\n",
    "        #first create new features: month, weekday, weeknum\n",
    "        df['year'] = pd.to_datetime(df['date']).dt.isocalendar().year\n",
    "        #first create new features: year, month, weekday, weeknum\n",
    "\n",
    "        #first create new features: month, weekday, weeknum\n",
    "\n",
    "        df['week_num'] = pd.to_datetime(df['date']).dt.isocalendar().week\n",
    "\n",
    "        #then encode the score as a new feature - not sure if we'll need it \n",
    "        df['score_day'] = df['score'].apply(lambda x: 'yes' if pd.notnull(x) == True else '')\n",
    "\n",
    "        #then start aggregating by fips, month, week_num\n",
    "        aggregated_data_train = df.groupby(['fips','year','week_num']).agg(\n",
    "                                        {'PRECTOT': ['mean'],\n",
    "                                        'PS': ['mean'],\n",
    "                                        'QV2M': ['mean'],\n",
    "                                        'T2M': ['mean'],\n",
    "                                        'T2MDEW': ['mean'],\n",
    "                                        'T2MWET': ['mean'],\n",
    "                                        'T2M_MAX': ['mean'],\n",
    "                                        'T2M_MIN': ['mean'],\n",
    "                                        'T2M_RANGE': ['mean'],\n",
    "                                         'TS': ['mean'],\n",
    "                                         'WS10M': ['mean'],\n",
    "                                         'WS10M_MAX': ['mean'],\n",
    "                                         'WS10M_MIN': ['mean'],\n",
    "                                         'WS10M_RANGE': ['mean'],\n",
    "                                         'WS50M': ['mean'],\n",
    "                                         'WS50M_MAX': ['mean'],\n",
    "                                         'WS50M_MIN': ['mean'],\n",
    "                                         'WS50M_RANGE': ['mean'],\n",
    "                                         'score': 'max'}).reset_index().sort_values(['fips','year','week_num'])\n",
    "\n",
    "        #finally, remove the multiindex from aggregated data_train so it looks neat and has flat column name structure\n",
    "        #Then round scores to nearest integer\n",
    "        aggregated_data_train.columns = ['_'.join(col) for col in aggregated_data_train.columns.values]\n",
    "        aggregated_data_train['score_max'] = aggregated_data_train['score_max'].map(lambda x: np.round(x))\n",
    "\n",
    "        fips_dict = self.fips_dict.drop(columns=['COUNTYNAME',\"STATE\",'geom']).rename(columns={'fips':'fips_'})\n",
    "        fips_dict[\"lat_long\"] = fips_dict[\"lat_long\"].transform(lambda x: ast.literal_eval(x))\n",
    "        fips_dict[\"lat\"] = pd.DataFrame(fips_dict[\"lat_long\"].tolist())[0]\n",
    "        fips_dict[\"long\"] = pd.DataFrame(fips_dict[\"lat_long\"].tolist())[1]\n",
    "        fips_dict.drop(columns=[\"lat_long\"],inplace=True)\n",
    "        aggregated_data_train = pd.merge(aggregated_data_train,fips_dict, on=[\"fips_\"], how=\"inner\")\n",
    "        aggregated_data_train = aggregated_data_train[['fips_', 'year_', 'week_num_', 'PRECTOT_mean', 'PS_mean', 'QV2M_mean',\n",
    "       'T2M_mean', 'T2MDEW_mean', 'T2MWET_mean', 'T2M_MAX_mean',\n",
    "       'T2M_MIN_mean', 'T2M_RANGE_mean', 'TS_mean', 'WS10M_mean',\n",
    "       'WS10M_MAX_mean', 'WS10M_MIN_mean', 'WS10M_RANGE_mean', 'WS50M_mean',\n",
    "       'WS50M_MAX_mean', 'WS50M_MIN_mean', 'WS50M_RANGE_mean','lat', 'long','score_max']]\n",
    "\n",
    "        return aggregated_data_train.dropna()\n",
    "    \n",
    "    def light_weekly_aggregate_validate(self):\n",
    "        df = self.validation_data\n",
    "        #first create new features: month, weekday, weeknum\n",
    "        df['year'] = pd.to_datetime(df['date']).dt.isocalendar().year\n",
    "        df['week_num'] = pd.to_datetime(df['date']).dt.isocalendar().week\n",
    "\n",
    "        #then encode the score as a new feature - not sure if we'll need it \n",
    "        df['score_day'] = df['score'].apply(lambda x: 'yes' if pd.notnull(x) == True else '')\n",
    "\n",
    "        #then start aggregating by fips, month, week_num\n",
    "        aggregated_data_validate = df.groupby(['fips','year','week_num']).agg(\n",
    "                                        {'PRECTOT': ['mean'],\n",
    "                                        'PS': ['mean'],\n",
    "                                        'QV2M': ['mean'],\n",
    "                                        'T2M': ['mean'],\n",
    "                                        'T2MDEW': ['mean'],\n",
    "                                        'T2MWET': ['mean'],\n",
    "                                        'T2M_MAX': ['mean'],\n",
    "                                        'T2M_MIN': ['mean'],\n",
    "                                        'T2M_RANGE': ['mean'],\n",
    "                                         'TS': ['mean'],\n",
    "                                         'WS10M': ['mean'],\n",
    "                                         'WS10M_MAX': ['mean'],\n",
    "                                         'WS10M_MIN': ['mean'],\n",
    "                                         'WS10M_RANGE': ['mean'],\n",
    "                                         'WS50M': ['mean'],\n",
    "                                         'WS50M_MAX': ['mean'],\n",
    "                                         'WS50M_MIN': ['mean'],\n",
    "                                         'WS50M_RANGE': ['mean'],\n",
    "                                         'score': 'max'}).reset_index().sort_values(['fips','year','week_num'])\n",
    "\n",
    "        #finally, remove the multiindex from aggregated data_train so it looks neat and has flat column name structure\n",
    "        #Then round scores to nearest integer\n",
    "        aggregated_data_validate.columns = ['_'.join(col) for col in aggregated_data_validate.columns.values]\n",
    "        aggregated_data_validate['score_max'] = aggregated_data_validate['score_max'].map(lambda x: np.round(x))\n",
    "        \n",
    "        fips_dict = self.fips_dict.drop(columns=['COUNTYNAME',\"STATE\",'geom']).rename(columns={'fips':'fips_'})\n",
    "        fips_dict[\"lat_long\"] = fips_dict[\"lat_long\"].transform(lambda x: ast.literal_eval(x))\n",
    "        fips_dict[\"lat\"] = pd.DataFrame(fips_dict[\"lat_long\"].tolist())[0]\n",
    "        fips_dict[\"long\"] = pd.DataFrame(fips_dict[\"lat_long\"].tolist())[1]\n",
    "        fips_dict.drop(columns=[\"lat_long\"],inplace=True)\n",
    "        aggregated_data_validate = pd.merge(aggregated_data_validate,fips_dict, on=[\"fips_\"], how=\"inner\")\n",
    "        aggregated_data_validate = aggregated_data_validate[['fips_', 'year_', 'week_num_', 'PRECTOT_mean', 'PS_mean', 'QV2M_mean',\n",
    "       'T2M_mean', 'T2MDEW_mean', 'T2MWET_mean', 'T2M_MAX_mean',\n",
    "       'T2M_MIN_mean', 'T2M_RANGE_mean', 'TS_mean', 'WS10M_mean',\n",
    "       'WS10M_MAX_mean', 'WS10M_MIN_mean', 'WS10M_RANGE_mean', 'WS50M_mean',\n",
    "       'WS50M_MAX_mean', 'WS50M_MIN_mean', 'WS50M_RANGE_mean','lat', 'long','score_max']]\n",
    "\n",
    "        return aggregated_data_validate.dropna()\n",
    "\n",
    "    def light_weekly_aggregate_test(self):\n",
    "        df = self.test_data\n",
    "        #first create new features: month, weekday, weeknum\n",
    "        df['year'] = pd.to_datetime(df['date']).dt.isocalendar().year\n",
    "        df['week_num'] = pd.to_datetime(df['date']).dt.isocalendar().week\n",
    "\n",
    "        #then encode the score as a new feature - not sure if we'll need it \n",
    "        df['score_day'] = df['score'].apply(lambda x: 'yes' if pd.notnull(x) == True else '')\n",
    "\n",
    "        #then start aggregating by fips, month, week_num\n",
    "        aggregated_data_test = df.groupby(['fips','year','week_num']).agg(\n",
    "                                        {'PRECTOT': ['mean'],\n",
    "                                        'PS': ['mean'],\n",
    "                                        'QV2M': ['mean'],\n",
    "                                        'T2M': ['mean'],\n",
    "                                        'T2MDEW': ['mean'],\n",
    "                                        'T2MWET': ['mean'],\n",
    "                                        'T2M_MAX': ['mean'],\n",
    "                                        'T2M_MIN': ['mean'],\n",
    "                                        'T2M_RANGE': ['mean'],\n",
    "                                         'TS': ['mean'],\n",
    "                                         'WS10M': ['mean'],\n",
    "                                         'WS10M_MAX': ['mean'],\n",
    "                                         'WS10M_MIN': ['mean'],\n",
    "                                         'WS10M_RANGE': ['mean'],\n",
    "                                         'WS50M': ['mean'],\n",
    "                                         'WS50M_MAX': ['mean'],\n",
    "                                         'WS50M_MIN': ['mean'],\n",
    "                                         'WS50M_RANGE': ['mean'],\n",
    "                                         'score': 'max'}).reset_index().sort_values(['fips','year','week_num'])\n",
    "\n",
    "        #finally, remove the multiindex from aggregated data_train so it looks neat and has flat column name structure\n",
    "        #Then round scores to nearest integer\n",
    "        aggregated_data_test.columns = ['_'.join(col) for col in aggregated_data_test.columns.values]\n",
    "        aggregated_data_test['score_max'] = aggregated_data_test['score_max'].map(lambda x: np.round(x))\n",
    "\n",
    "        fips_dict = self.fips_dict.drop(columns=['COUNTYNAME',\"STATE\",'geom']).rename(columns={'fips':'fips_'})\n",
    "        fips_dict[\"lat_long\"] = fips_dict[\"lat_long\"].transform(lambda x: ast.literal_eval(x))\n",
    "        fips_dict[\"lat\"] = pd.DataFrame(fips_dict[\"lat_long\"].tolist())[0]\n",
    "        fips_dict[\"long\"] = pd.DataFrame(fips_dict[\"lat_long\"].tolist())[1]\n",
    "        fips_dict.drop(columns=[\"lat_long\"],inplace=True)\n",
    "        aggregated_data_test = pd.merge(aggregated_data_test,fips_dict, on=[\"fips_\"], how=\"inner\")\n",
    "        aggregated_data_test = aggregated_data_test[['fips_', 'year_', 'week_num_', 'PRECTOT_mean', 'PS_mean', 'QV2M_mean',\n",
    "       'T2M_mean', 'T2MDEW_mean', 'T2MWET_mean', 'T2M_MAX_mean',\n",
    "       'T2M_MIN_mean', 'T2M_RANGE_mean', 'TS_mean', 'WS10M_mean',\n",
    "       'WS10M_MAX_mean', 'WS10M_MIN_mean', 'WS10M_RANGE_mean', 'WS50M_mean',\n",
    "       'WS50M_MAX_mean', 'WS50M_MIN_mean', 'WS50M_RANGE_mean','lat', 'long','score_max']]\n",
    "\n",
    "        return aggregated_data_test.dropna()\n",
    "\n",
    "   \n",
    "    def k_best_features(self):\n",
    "        df = self.light_weekly_aggregate()\n",
    "    \n",
    "        y = round(df['score_max'])\n",
    "        X = df.drop(columns=['fips_','week_num_','score_max'])\n",
    "        \n",
    "        k_best_f = SelectKBest(f_classif, k=10).fit(X, y)\n",
    "        df_scores = pd.DataFrame({'features': X.columns, 'ANOVA F-value': k_best_f.scores_, 'pValue': k_best_f.pvalues_ })\n",
    "\n",
    "        return df_scores.sort_values('ANOVA F-value', ascending=False).reset_index()\n",
    "    \n",
    "    def tree_feature_importance(self):\n",
    "        df = self.light_weekly_aggregate()\n",
    "    \n",
    "        y = round(df['score_max'])\n",
    "        X = df.drop(columns=['fips_','week_num_','score_max'])\n",
    "        \n",
    "        tree_clf = DecisionTreeClassifier(max_depth=6, random_state=2)\n",
    "        tree_clf.fit(X,y)\n",
    "\n",
    "        return pd.DataFrame({'features': X.columns, 'Feature Importance': tree_clf.feature_importances_})\\\n",
    "            .sort_values('Feature Importance', ascending=False).iloc[:20]\n",
    "            \n",
    "    def return_lagged_function(self, weeks_back=5):\n",
    "        \n",
    "        df = self.light_weekly_aggregate()\n",
    "        \n",
    "        top_features = ['T2M_RANGE_mean', 'PS_mean', 'T2M_MAX_mean', 'TS_mean', \n",
    "                        'T2MDEW_mean', 'QV2M_mean', 'WS10M_MAX_mean', 'PRECTOT_mean']\n",
    "        \n",
    "        all_features = [i for i in df.columns if i in top_features or i in ['fips_', 'year_', 'week_num_']]\n",
    "        \n",
    "        df_processed = df[all_features]\n",
    "        \n",
    "        for e in top_features:\n",
    "            for i in range(1, weeks_back):\n",
    "                df_processed[f'{e} - {i}'] = df_processed.groupby(['fips_'])[f'{e}'].shift(i)\n",
    "                \n",
    "    \n",
    "       #return df_processed \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "droughts",
   "language": "python",
   "name": "droughts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
